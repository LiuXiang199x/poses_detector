// Generated by gencpp from file etts_msgs/UtteranceGoal.msg
// DO NOT EDIT!


#ifndef ETTS_MSGS_MESSAGE_UTTERANCEGOAL_H
#define ETTS_MSGS_MESSAGE_UTTERANCEGOAL_H


#include <string>
#include <vector>
#include <map>

#include <ros/types.h>
#include <ros/serialization.h>
#include <ros/builtin_message_traits.h>
#include <ros/message_operations.h>

#include <etts_msgs/Utterance.h>

namespace etts_msgs
{
template <class ContainerAllocator>
struct UtteranceGoal_
{
  typedef UtteranceGoal_<ContainerAllocator> Type;

  UtteranceGoal_()
    : utterance()  {
    }
  UtteranceGoal_(const ContainerAllocator& _alloc)
    : utterance(_alloc)  {
  (void)_alloc;
    }



   typedef  ::etts_msgs::Utterance_<ContainerAllocator>  _utterance_type;
  _utterance_type utterance;





  typedef boost::shared_ptr< ::etts_msgs::UtteranceGoal_<ContainerAllocator> > Ptr;
  typedef boost::shared_ptr< ::etts_msgs::UtteranceGoal_<ContainerAllocator> const> ConstPtr;

}; // struct UtteranceGoal_

typedef ::etts_msgs::UtteranceGoal_<std::allocator<void> > UtteranceGoal;

typedef boost::shared_ptr< ::etts_msgs::UtteranceGoal > UtteranceGoalPtr;
typedef boost::shared_ptr< ::etts_msgs::UtteranceGoal const> UtteranceGoalConstPtr;

// constants requiring out of line definition



template<typename ContainerAllocator>
std::ostream& operator<<(std::ostream& s, const ::etts_msgs::UtteranceGoal_<ContainerAllocator> & v)
{
ros::message_operations::Printer< ::etts_msgs::UtteranceGoal_<ContainerAllocator> >::stream(s, "", v);
return s;
}

} // namespace etts_msgs

namespace ros
{
namespace message_traits
{



// BOOLTRAITS {'IsFixedSize': False, 'IsMessage': True, 'HasHeader': False}
// {'std_msgs': ['/opt/ros/kinetic/share/std_msgs/cmake/../msg'], 'actionlib_msgs': ['/opt/ros/kinetic/share/actionlib_msgs/cmake/../msg'], 'etts_msgs': ['/home/haobing/catkin_ws/src/etts_msgs/msg', '/home/haobing/catkin_ws/devel/share/etts_msgs/msg']}

// !!!!!!!!!!! ['__class__', '__delattr__', '__dict__', '__doc__', '__eq__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_parsed_fields', 'constants', 'fields', 'full_name', 'has_header', 'header_present', 'names', 'package', 'parsed_fields', 'short_name', 'text', 'types']




template <class ContainerAllocator>
struct IsFixedSize< ::etts_msgs::UtteranceGoal_<ContainerAllocator> >
  : FalseType
  { };

template <class ContainerAllocator>
struct IsFixedSize< ::etts_msgs::UtteranceGoal_<ContainerAllocator> const>
  : FalseType
  { };

template <class ContainerAllocator>
struct IsMessage< ::etts_msgs::UtteranceGoal_<ContainerAllocator> >
  : TrueType
  { };

template <class ContainerAllocator>
struct IsMessage< ::etts_msgs::UtteranceGoal_<ContainerAllocator> const>
  : TrueType
  { };

template <class ContainerAllocator>
struct HasHeader< ::etts_msgs::UtteranceGoal_<ContainerAllocator> >
  : FalseType
  { };

template <class ContainerAllocator>
struct HasHeader< ::etts_msgs::UtteranceGoal_<ContainerAllocator> const>
  : FalseType
  { };


template<class ContainerAllocator>
struct MD5Sum< ::etts_msgs::UtteranceGoal_<ContainerAllocator> >
{
  static const char* value()
  {
    return "c53758c81c169668f7aa3e3470102d6f";
  }

  static const char* value(const ::etts_msgs::UtteranceGoal_<ContainerAllocator>&) { return value(); }
  static const uint64_t static_value1 = 0xc53758c81c169668ULL;
  static const uint64_t static_value2 = 0xf7aa3e3470102d6fULL;
};

template<class ContainerAllocator>
struct DataType< ::etts_msgs::UtteranceGoal_<ContainerAllocator> >
{
  static const char* value()
  {
    return "etts_msgs/UtteranceGoal";
  }

  static const char* value(const ::etts_msgs::UtteranceGoal_<ContainerAllocator>&) { return value(); }
};

template<class ContainerAllocator>
struct Definition< ::etts_msgs::UtteranceGoal_<ContainerAllocator> >
{
  static const char* value()
  {
    return "# ====== DO NOT MODIFY! AUTOGENERATED FROM AN ACTION DEFINITION ======\n\
#Goal definition: input parameters\n\
Utterance utterance\n\
\n\
================================================================================\n\
MSG: etts_msgs/Utterance\n\
####\n\
#### the messages sent to use the ETTS engine, via the topic \"etts\"\n\
####\n\
#### Apart from the info contained in this kind of message,\n\
#### these ROS parameters, written by the skill, can be useful for information:\n\
#### \"etts_language\" :  for example \"es\"\n\
#### \"etts_emotion\" :   for example \"happy\"\n\
#### \"etts_primitive\" : for example \"google\"\n\
#### \"etts_volume\" : integer in [VOLUME_MIN, VOLUME_MAX]\n\
#### \"etts_queue_size\" : for example \"2\"\n\
#### \"etts_speaking_now\" : true or false, set at the beginning and end of each sentence\n\
#### \"etts_current_sentence\": the sentence given to the low level primitive, for instance \"Hola\"\n\
####\n\
#### And the parameters written by the different APIs:\n\
#### \"etts_kidnapped\" : for example \"\" (not kidnapped) or \"node_name\"\n\
\n\
\n\
### useful for the stamp\n\
Header header\n\
\n\
### the real sentence to be said\n\
# A simple example: text=\"Hello world, how are you?\";\n\
#\n\
# -> if using FESTIVAL|GOOGLE|LOQUENDO|MICROSOFT|PICO|ESPEAK|NUANCE|IVONA|AT primitive:\n\
#    multi-languages support\n\
#    Example: text=\"es:hola|en:hi|en:Hello|fr:bonjour\";\n\
#    Multiple instances of a given language are supported,\n\
#    one of these will be chosen randomly.\n\
#    Example:  textMultiLanguage=\"en:Hello|en:Hi|fr:Salut\";\n\
#    if (target_language == LANGUAGE_ENGLISH), will return randomly \"Hello\" or \"Hi\"\n\
#    if (target_language == LANGUAGE_FRENCH), will return \"Salut\"\n\
#    (parsing is made in EttsSkill::process_utterance()\n\
#      -> Translator::_build_given_language_in_multilanguage_line() )\n\
#\n\
# -> if using FESTIVAL|GOOGLE|LOQUENDO|MICROSOFT|PICO|ESPEAK|NUANCE|IVONA|AT primitive:\n\
#    snippets support\n\
#    Example: text=\"Yes, \\\\NLG=OK let us do it\";\n\
#    replace the natural language snippets with their equivalent\n\
#    (parsing is made in EttsSkill::process_utterance()\n\
#      -> utterance_utils::replace_natural_langugage_tags() )\n\
#\n\
# -> if using FESTIVAL|GOOGLE|LOQUENDO|MICROSOFT|PICO|ESPEAK|NUANCE|IVONA|AT primitive:\n\
#    metadata tags support\n\
#    Emotion flags:\n\
#    Example: text=\"\\emotion=Happy I speak with a happy voice.\";\n\
#    Among \"HAPPY\", \"SAD\", \"TRANQUILITY\", \"NERVOUS\"\n\
#    (parsing is made in EttsSkill::process_utterance()\n\
#      -> EttsSkill::apply_metadata_tags() )\n\
#\n\
# -> if using LOQUENDO primitive: additional flags\n\
#    The Loquendo engine support a number of additional flags.\n\
#    Example: text=\"I had... \\\\pause=500 a complete relooking. \\\\item=Whistle_01\";\n\
#    These tags are stripped out of the sentence if another engine is used.\n\
#    (parsing is made in EttsSkill::process_utterance()\n\
#      -> utterance_utils::strip_metadata_tags() )\n\
#\n\
# -> if using NONVERBAL primitive: keys\n\
#    You must use one of the semantic keys of non_verbal.h:\n\
#    \"SINGING\"|\"CONFIRMATION\"|\"THINKING\"|\"WARNING\"|\"DIALOG\"|\"HELLO\"|\"ERROR\"|\"AMAZING\"\n\
#    Example: text=\"SINGING\";\n\
#\n\
# -> if using  MUSIC_SCORE primitive, use a music score (cf etts_music_score.h)\n\
#    Example:  text=\"BPM=85,A6,{},Ab2,C5,1/4-C3\";\n\
#    You can also use a filepath, it must end with \".score\".\n\
#    For example text=\"/tmp/mymusic.score\".\n\
#    This file must contain a valid music score, for instance \"C4,D4,E4,D4,C4\"\n\
#\n\
# Note that if using NONVERBAL and MUSIC_SCORE,\n\
#   multi-language, metadata, snippets are not used.\n\
#   However, the language and the emotion fields of the message are applied as usual.\n\
string text\n\
\n\
### language domain\n\
# \"en\" for english, \"es\" for spanish, etc.\n\
# empty (\"\") for keeping the same language\n\
string language\n\
\n\
# re-use the same primitive that was used in the last sentence\n\
int16 PRIM_LAST_USED = 0\n\
# open source, homebrew - does not emit any sound (mute)\n\
int16 PRIM_NOVOICE = 1\n\
# open source - based on the Festival project, by the University of Edinburgh\n\
# http://www.cstr.ed.ac.uk/projects/festival/\n\
int16 PRIM_FESTIVAL = 2\n\
# prorietary, webservice - Google TTS webservice, most notably used in Google Translate\n\
# https://en.wikipedia.org/wiki/Google_Text-to-Speech\n\
int16 PRIM_GOOGLE = 3\n\
# prorietary - based on the Loquendo binaries\n\
# http://www.nuance.com/for-business/customer-service-solutions/loquendo-small-business-bundle/index.htm\n\
int16 PRIM_LOQUENDO = 4\n\
# prorietary, webservice - the speech synthesizer developed by Microsoft for its OS\n\
# https://en.wikipedia.org/wiki/Microsoft_text-to-speech_voices\n\
int16 PRIM_MICROSOFT = 5\n\
# open source, homebrew - beep and other sounds\n\
int16 PRIM_NONVERBAL = 6\n\
# open source, homebrew - generate music sounds based the music score, written in notes\n\
int16 PRIM_MUSIC_SCORE = 7\n\
# open source - based on pico2wave, a small footprint TTS released by SVOX\n\
# https://en.wikipedia.org/wiki/SVOX\n\
int16 PRIM_PICO = 8\n\
# open source - based on the espeak utility, a multi-lingual software speech synthesizer\n\
# http://espeak.sourceforge.net/\n\
int16 PRIM_ESPEAK = 9\n\
# prorietary, webservice - uses the mobile Nuance Mobility webservice\n\
# http://www.nuance.com/for-business/customer-service-solutions/loquendo-small-business-bundle/index.htm\n\
int16 PRIM_NUANCE = 10\n\
# prorietary, webservice - uses the IVONA TTS webservice for developpers\n\
# http://developer.ivona.com/en/speechcloud/index.html\n\
int16 PRIM_IVONA = 11\n\
# prorietary, webservice - AT&T TTS webservice\n\
# http://www2.research.att.com/~ttsweb/tts/\n\
int16 PRIM_AT = 12\n\
# prorietary\n\
# https://www.readspeaker.com\n\
int16 PRIM_READSPEAKER = 13\n\
\n\
# the wanted low-level voice engine: one of the previous PRIM_*\n\
# leave empty (PRIM_LAST_USED) for keeping the same primitive\n\
int16 primitive\n\
\n\
\n\
#The identifier of the utterance in oder to know when the utterance begins and ends.\n\
#This identifier  could be the miliseconds of the system, in this way it's unequivocal\n\
#To get the identifier you can use this boost funtion:  boost::posix_time::second_clock::local_time();\n\
int64 utteranceIdentifier\n\
\n\
\n\
\n\
int16 EMOTION_LAST_USED = 0\n\
int16 EMOTION_HAPPY = 1\n\
int16 EMOTION_SAD = 2\n\
int16 EMOTION_NEUTRAL = 3\n\
int16 EMOTION_ANXIOUS = 4\n\
int16 EMOTION_RELAXED = 5\n\
int16 EMOTION_ANGRY = 6\n\
int16 EMOTION_BORED = 7\n\
int16 EMOTION_SURPRISED = 8\n\
\n\
### the wanted emotion, if supported by the primitive\n\
# use one of the previous EMOTION_*\n\
# leave empty (EMOTION_LAST_USED) for keeping the same emotion\n\
int16 emotion\n\
\n\
\n\
\n\
### the parameters that configure the voice (ALPHA VERSION)\n\
int16 paralinguistic\n\
int16 pitch\n\
float32 prosody_rate\n\
\n\
\n\
\n\
int16 VOLUME_LAST_USED = 0\n\
int16 VOLUME_MIN = 1\n\
int16 VOLUME_MAX = 100\n\
### the desired volume output, if supported by the primitive\n\
# volume must be between VOLUME_MIN and VOLUME_MAX\n\
# leave empty (VOLUME_LAST_USED) for no change\n\
int16 volume\n\
\n\
int16 QUEUE_SENTENCE = 0\n\
int16 SHUTUP_IMMEDIATLY_AND_SAY_SENTENCE = 1\n\
int16 SHUTUP_AND_SAY_SENTENCE = 2\n\
int16 PAUSE = 3\n\
int16 RESUME = 4\n\
int16 SPEAK_ONLY_IF_ROBOT_QUIET = 5\n\
### by default, queue the sentence and say it when the previous ones have been said.\n\
#  Use one of the previous orders to change this behaviour.\n\
#  Note that if using the special instructions PAUSE and RESUME,\n\
#  all the other fields of the message (including text) are discarded.\n\
int16 priority\n\
\n\
================================================================================\n\
MSG: std_msgs/Header\n\
# Standard metadata for higher-level stamped data types.\n\
# This is generally used to communicate timestamped data \n\
# in a particular coordinate frame.\n\
# \n\
# sequence ID: consecutively increasing ID \n\
uint32 seq\n\
#Two-integer timestamp that is expressed as:\n\
# * stamp.sec: seconds (stamp_secs) since epoch (in Python the variable is called 'secs')\n\
# * stamp.nsec: nanoseconds since stamp_secs (in Python the variable is called 'nsecs')\n\
# time-handling sugar is provided by the client library\n\
time stamp\n\
#Frame this data is associated with\n\
# 0: no frame\n\
# 1: global frame\n\
string frame_id\n\
";
  }

  static const char* value(const ::etts_msgs::UtteranceGoal_<ContainerAllocator>&) { return value(); }
};

} // namespace message_traits
} // namespace ros

namespace ros
{
namespace serialization
{

  template<class ContainerAllocator> struct Serializer< ::etts_msgs::UtteranceGoal_<ContainerAllocator> >
  {
    template<typename Stream, typename T> inline static void allInOne(Stream& stream, T m)
    {
      stream.next(m.utterance);
    }

    ROS_DECLARE_ALLINONE_SERIALIZER
  }; // struct UtteranceGoal_

} // namespace serialization
} // namespace ros

namespace ros
{
namespace message_operations
{

template<class ContainerAllocator>
struct Printer< ::etts_msgs::UtteranceGoal_<ContainerAllocator> >
{
  template<typename Stream> static void stream(Stream& s, const std::string& indent, const ::etts_msgs::UtteranceGoal_<ContainerAllocator>& v)
  {
    s << indent << "utterance: ";
    s << std::endl;
    Printer< ::etts_msgs::Utterance_<ContainerAllocator> >::stream(s, indent + "  ", v.utterance);
  }
};

} // namespace message_operations
} // namespace ros

#endif // ETTS_MSGS_MESSAGE_UTTERANCEGOAL_H
